{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Search engine Query Parser.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "Zloko4oixQEs"
      },
      "outputs": [],
      "source": [
        "def importingLibraries():\n",
        " \n",
        "  import nltk\n",
        "  nltk.download('punkt')\n",
        "  nltk.download('stopwords')\n",
        "  nltk.download('wordnet')\n",
        "  \n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def readingPickle():\n",
        "   import pickle\n",
        "   with open('index.pkl', 'rb') as fid:\n",
        "    data3 = pickle.load(fid)\n",
        "    return data3"
      ],
      "metadata": {
        "id": "WZu_E9anzKX7"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def readingQuery():\n",
        "  query=input(\"How can I help You: \")\n",
        "  return query\n"
      ],
      "metadata": {
        "id": "m5sfHqQc1ZXM"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenizingQuery(searchInput):\n",
        "   from nltk.tokenize import word_tokenize as sentence2words\n",
        "   from nltk.tokenize import sent_tokenize as paragraph2sentence\n",
        "   words = sentence2words(searchInput)\n",
        "   return words"
      ],
      "metadata": {
        "id": "wxRKKgGi2PFe"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def standarizingQuery(words):\n",
        "  standarizedWords=[]\n",
        "  for word in words:\n",
        "    standarizedWord=word.lower()\n",
        "    standarizedWords.append(standarizedWord)\n",
        "  return standarizedWords"
      ],
      "metadata": {
        "id": "TB2YvpFG3JD0"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def removingStopWords(standarizedWords):\n",
        "    from nltk.corpus import stopwords\n",
        "    stopWords = set(stopwords.words('english'))\n",
        "    wordsFiltered = []\n",
        "    for standarizedWord in standarizedWords:\n",
        "         if (standarizedWord not in stopWords) and (standarizedWord.isalnum()):\n",
        "           wordsFiltered.append(standarizedWord)\n",
        "    return wordsFiltered\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NyQS-qmW34Vs"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def stemTokens(wordsFiltered):\n",
        "  from nltk.stem import PorterStemmer\n",
        "  from nltk.stem import LancasterStemmer\n",
        "  #create an object of class PorterStemmer\n",
        "  porter = PorterStemmer()\n",
        "  lancaster=LancasterStemmer()\n",
        "  stemmedWords=[]\n",
        "  for wordFiltered in wordsFiltered:\n",
        "    #porter stemmer was more suitable for this application, so it was conucted. you can check this valiation by uncommenting the following line\n",
        "    #print(\"{0:20}{1:20}{2:20}\".format(wordFiltered,porter.stem(wordFiltered),lancaster.stem(wordFiltered)))\n",
        "    stemmedWord=porter.stem(wordFiltered)\n",
        "    stemmedWords.append(stemmedWord)\n",
        "  return stemmedWords\n"
      ],
      "metadata": {
        "id": "V66E45Y65M6A"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmTokens(stemmedWords):\n",
        "  from nltk.stem import WordNetLemmatizer\n",
        "  wordnet_lemmatizer = WordNetLemmatizer()\n",
        "  lemmitizedWords=[]\n",
        "  for stemmedWord in stemmedWords:\n",
        "    lemmitizedWord=wordnet_lemmatizer.lemmatize(stemmedWord)\n",
        "    lemmitizedWords.append(lemmitizedWord)\n",
        "  return(lemmitizedWords)"
      ],
      "metadata": {
        "id": "EXa2a71R6Qg2"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieveDocuments(lemmitizedWords,data3):\n",
        "  detailedOutput=[]\n",
        "  for i in range(len(lemmitizedWords)):    \n",
        "     var=lemmitizedWords[i]\n",
        "     detailedOutputUnit=data3.get(str(var), \"sorry no document is found\")\n",
        "     detailedOutput.append(detailedOutputUnit)\n",
        "  documentList=[]\n",
        "  for i in range(len(detailedOutput)):    \n",
        "           documentList+= detailedOutput[i]\n",
        "  documentListWithNoRepetition = list(dict.fromkeys([x[0] for x in documentList]))\n",
        "  print('we found these documents according to your search')\n",
        "  \n",
        "  print('count of found documents is=',len(documentListWithNoRepetition))\n",
        "  for i in range(len(documentListWithNoRepetition)):  \n",
        "      print('document number=', documentListWithNoRepetition[i])\n",
        "  \n",
        "\n"
      ],
      "metadata": {
        "id": "Si8jskju65mT"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ˜‰ðŸ˜ŽðŸŽ‰\n",
        "# **search engine code:** "
      ],
      "metadata": {
        "id": "qFHWBmu3_8e2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Who were famous women?\n",
        "#Poems and poets\n",
        "#What is the scientific work of islamic golden age?\n",
        "#Ancient Spain\n",
        "#A Famous Muslim Engineer\n",
        "#A woman with a kingdom\n",
        "#How were the prayer times determined?\n",
        "#A theologian\n",
        "importingLibraries()\n",
        "referenceDict=readingPickle()\n",
        "searchedQuery=readingQuery()\n",
        "words=tokenizingQuery(searchedQuery)\n",
        "standarizedWords=standarizingQuery(words)\n",
        "wordsFiltered=removingStopWords(standarizedWords)\n",
        "stemmedWords=stemTokens(wordsFiltered)\n",
        "lemmitizedWords=lemmTokens(stemmedWords)\n",
        "retrieveDocuments(lemmitizedWords,referenceDict)\n",
        "#enter the queries as normal user using the search bar that is going to show when running.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMbHoKtR0Sis",
        "outputId": "2d538871-0ab8-4938-9dea-370e6dd4e58b"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "How can I help You: #A theologian\n",
            "we found these documents according to your search\n",
            "count of found documents is= 1\n",
            "document number= 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Assesments Questions***"
      ],
      "metadata": {
        "id": "4J1F3PkbfJtd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " \n",
        "*   'A theologian' Query\n",
        "*   'A Famous Muslim Engineer'\n",
        "*    'Who were famous women?', 'A theologian', 'Poems and poets', 'What is the scientific work of islamic golden age?'\n",
        "\n",
        "*   Ancient Spain, A Famous Muslim Engineer, A woman with a kingdom, How were the prayer times determined?\n",
        "\n",
        "*   because the code does not epend on the occurence of the searched words inside the document, it lacks the descision making of whether taking this document or not regarding the occurence. it does not give higher weights for the most important words in the query which le to retrieve any matched documnets regardless its importance to the query. it does not search for the document that joining many words from the query. it lacks mini semantics that can power the deocuments retrieval descisions. \n",
        "\n",
        "*   Abu Muhammad Izz al-Din, i did not know that he is the one who Al-Dhahabi described as someone who attained the rank of ijtihad, with asceticism and piety and the command of virtue and forbidding of what is evil and solidity in religion.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TxVw6oIO-LAV"
      }
    }
  ]
}